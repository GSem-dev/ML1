{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Считывание файла (Не в pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_titles = {}\n",
    "def one_line(line):\n",
    "    data = line.split('\\t', 1)\n",
    "    doc_id = int(data[0])\n",
    "    if len(data[1]) == 1:\n",
    "        title = ''\n",
    "    else:\n",
    "        title = data[1]\n",
    "    docs_titles[doc_id] = title[:-1] # без \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/docs_titles.tsv', 'r') as f:\n",
    "    l = f.readline()\n",
    "    for line in f:\n",
    "        one_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28026"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в качестве предобработки избавимся от:\n",
    "\n",
    "- верхнего регистра\n",
    "- не русских и не английских букв и не цифр\n",
    "- не будем рассматривать the a ... стоп слова\n",
    "- фильтруем слова по-умному: stemmer (лемматизация хуже)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/snap/jupyter/common/lib/python3.7/site-packages/joblib/_multiprocessing_helpers.py:45: UserWarning: [Errno 13] Permission denied.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymorphy2\n",
    "# morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/simon/snap/jupyter/6/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words([\"russian\", \"english\"]))\n",
    "stemmerR = SnowballStemmer(\"russian\")\n",
    "stemmerE = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_data = {}\n",
    "def title_process(title_id):\n",
    "    title = docs_titles[title_id].lower()\n",
    "    tmp = [\n",
    "#         morph.parse(x.lower())[0].normal_form for x in re.sub('[^0-9a-zа-я]', ' ', title).split()\n",
    "        stemmerR.stem(stemmerE.stem(x)) for x in re.sub('[^0-9a-zа-я]', ' ', title).split()\n",
    "        if not x in stop_words\n",
    "    ]\n",
    "    titles_data[title_id] = ' '.join([x for x in tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in docs_titles:\n",
    "    title_process(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28026"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# titles_data\n",
    "len(titles_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ контента страниц "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "это нужно сделать параллельно\n",
    "\n",
    "нужно сохранить распарсенные данные для упрощения работы\n",
    "\n",
    "... слишком много данных, ощуение что одних заголовков должно хватить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "from multiprocessing.dummy import Lock as ThreadLock \n",
    "from multiprocessing.dummy import Value as ThreadValue\n",
    "import functools\n",
    "import codecs\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel(func):\n",
    "    mutex = ThreadLock()\n",
    "    n_thread = ThreadValue('i',0)\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **argv):\n",
    "        result = func(*args, **argv)\n",
    "        with mutex:\n",
    "            nonlocal n_thread\n",
    "            n_thread.value +=1\n",
    "            print(f\"\\r{n_thread.value} ready\",end ='',flush = True)\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {}\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "@parallel\n",
    "def parser(n):\n",
    "#     lit=set('.,?!абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ')\n",
    "    file_path = 'data/content/{}.dat'.format(n)\n",
    "    f = codecs.open(file_path, encoding='utf-8')\n",
    "    soup = BeautifulSoup(f, 'lxml')\n",
    "    content = soup.find_all('div') #  для группировки контента\n",
    "    result = []\n",
    "    for txt in content:\n",
    "        cnt = 0\n",
    "        for s in txt.stripped_strings:\n",
    "            if cnt > 66:\n",
    "                break\n",
    "            for w in s.split():\n",
    "                if len(result) > 200:\n",
    "                    break\n",
    "                if w.lower() not in stop_words:\n",
    "                    result.append(morph.parse(w.lower())[0].normal_form)\n",
    "        #                 result.append(stemmerR.stem(t.lower()))\n",
    "    documents[n] = ' '.join(result)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ThreadPool(10) as pool: \n",
    "#     pool.map(parser, list(range(1,28027)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = 'data/content/{}.dat'.format(6)#6265\n",
    "# f = codecs.open(file_path, encoding='utf-8')\n",
    "# soup = BeautifulSoup(f, 'lxml')\n",
    "# content = soup.find_all('div')\n",
    "# # print(content[0].text.split())\n",
    "# # print(soup)\n",
    "# for x in content[3].strings:\n",
    "#     if re.search(r'\\s', x):\n",
    "#         continue\n",
    "#     print(x.strip())\n",
    "# # print(content[3])\n",
    "# f.close()\n",
    "\n",
    "# # re.searc(r'\\s', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def parser_stopwords(texts_dict):\n",
    "    words_dict = {}\n",
    "    for ID, text in tqdm(texts_dict.items()):\n",
    "        words_dict[ID] = ' '.join([word for word in text.split() if not word in stop_words])\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_nsw = parser_stopwords(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt_list = list(documents_nsw.values())\n",
    "df = pd.DataFrame.from_dict(\n",
    "    documents_nsw, orient='index', columns=['txt']\n",
    ")\n",
    "df['ind']=df.index\n",
    "df = df.sort_values('ind')\n",
    "df = df.drop(['ind'], axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('text_lemm.csv', encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обратно в список\n",
    "# a = pd.read_csv('texts/text_result_plus_stemm.csv')\n",
    "# doc_to_text={}\n",
    "# for i in range(1, 28027):\n",
    "#     doc_to_text[i]=a.txt[i-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ищу фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cтандартизацию стоит применять при использование алгоритмов, которые основываются на измерении расстояний, например, k ближайших соседей или метод опорных векторов. Значит применяем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train_groups.csv')\n",
    "# пререгоняем в словарь\n",
    "#  по группам\n",
    "traingroups = {}\n",
    "for i in range(len(train_data)):\n",
    "    tmp = train_data.iloc[i]\n",
    "    doc_group = tmp['group_id']\n",
    "    doc_id = tmp['doc_id']\n",
    "    doc_target = tmp['target']\n",
    "    title = titles_data[doc_id]\n",
    "    if doc_group not in traingroups:\n",
    "        traingroups[doc_group] = []\n",
    "    traingroups[doc_group].append((doc_id, title, doc_target))\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "groups_train = []\n",
    "\n",
    "# общие слова\n",
    "for grp in traingroups:\n",
    "    docs = traingroups[grp] #  все тексты из группы\n",
    "    for k, (doc_id, title, doc_target) in enumerate(docs):\n",
    "        y_train.append(doc_target)\n",
    "        groups_train.append(grp)\n",
    "        words = set(title.strip().split())\n",
    "        common = []\n",
    "        for j in range(len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, doc_target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            common.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(common)[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 20) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "traingroups = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, traingroups.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test_groups.csv')\n",
    "testgroups = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    title = titles_data[doc_id]\n",
    "    if doc_group not in testgroups:\n",
    "        testgroups[doc_group] = []\n",
    "    testgroups[doc_group].append((doc_id, title))\n",
    "    \n",
    "X_test = []\n",
    "groups_test = []\n",
    "for grp in testgroups:\n",
    "    docs = testgroups[grp]\n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "        groups_train.append(grp)\n",
    "        words = set(title.strip().split())\n",
    "        common = []\n",
    "        for j in range(len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            common.append(len(words.intersection(words_j)))\n",
    "        X_test.append(sorted(common)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 20) (0,)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(X_test)\n",
    "testgroups = np.array(groups_test)\n",
    "print (X_test.shape, testgroups.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 20) (16627, 20)\n"
     ]
    }
   ],
   "source": [
    "scale_features_std = StandardScaler()\n",
    "X_train=scale_features_std.fit_transform(X_train)\n",
    "X_test=scale_features_std.fit_transform(X_test)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем что то еще"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn) \n",
    "- tfidf хорошо себя зарекомендовал в 1 дз\n",
    "- так же как и cos метрика\n",
    "- для метрики используем стандартизатор\n",
    "\n",
    "\n",
    "tree) \n",
    "- для деревъев себя лучше проявляет нормализатор\n",
    "\n",
    "\n",
    "svc)\n",
    "- тут опять же используется стандартизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class container:\n",
    "    def __init__(self, titles, features):\n",
    "        vec = TfidfVectorizer()\n",
    "        titles[0] = '' #  нулевого нет\n",
    "        self.doc_tfidf = vec.fit_transform([titles[i] for i in range(len(titles))])\n",
    "        tmp = {}\n",
    "        for i in titles.keys():\n",
    "            tmp[i] = titles[i].split(' ')\n",
    "        self.doc = tmp\n",
    "        self.n_features = features\n",
    "        self.metric = self.cosine\n",
    "            \n",
    "    def cosine(self, group):\n",
    "        n = self.n_features //2\n",
    "        X = np.ones(shape=(group.size, self.n_features), dtype=np.float)\n",
    "        for i, title in enumerate(pairwise_distances(self.doc_tfidf[group], metric='cosine')):\n",
    "            X[i, :n] = sorted(title)[1:n+1]\n",
    "        X[:, n:] = np.mean(X[:, :n], axis=0)\n",
    "#         X[:, :n] /= X[:, n:]\n",
    "        return X\n",
    "    \n",
    "    def read(self, file, dbsc_eps=0.5, dbsc_min_samples=5):\n",
    "        data = pd.read_csv(file)\n",
    "        groups = data.groupby('group_id')\n",
    "        if 'target' in data.columns:\n",
    "            X = np.zeros(shape=(data.shape[0], self.n_features+1), dtype=np.float)\n",
    "            y = np.zeros(shape=(data.shape[0], ), dtype=bool)\n",
    "            group_ids = np.zeros(shape=(data.shape[0], ), dtype=int)\n",
    "            i = 0\n",
    "            for group_id, group_index in groups.groups.items():\n",
    "                j = i + group_index.size # выделяю группу\n",
    "                group = data.iloc[group_index]\n",
    "                group_ids[i:j] = group_id\n",
    "                clf = DBSCAN(eps=dbsc_eps, metric=\"cosine\", min_samples=dbsc_min_samples) # крут\n",
    "                res = clf.fit_predict(self.doc_tfidf[group.doc_id])\n",
    "                for t in range(len(res)):\n",
    "                    if res[t] == -1:\n",
    "                        res[t] = 0\n",
    "                    else:\n",
    "                        res[t] = 1\n",
    "                y[i:j] = group.target\n",
    "                X[i:j,:-1] = self.metric(group.doc_id)\n",
    "                X[i:j,-1] = res\n",
    "                i = j\n",
    "            return X, y, group_ids\n",
    "        else:\n",
    "            X = np.zeros(shape=(data.shape[0], self.n_features+1), dtype=np.float)\n",
    "            pair_ids = np.zeros(shape=(data.shape[0], ), dtype=int)\n",
    "            i = 0\n",
    "            for group_id, group_index in groups.groups.items():\n",
    "                j = i + group_index.size\n",
    "                group = data.iloc[group_index]\n",
    "                pair_ids[i:j] = group.pair_id\n",
    "                clf = DBSCAN(0.5, metric=\"cosine\", min_samples=8)\n",
    "                res = clf.fit_predict(self.doc_tfidf[group.doc_id])\n",
    "                for t in range(len(res)):\n",
    "                    if res[t] == -1:\n",
    "                        res[t] = 0\n",
    "                    else:\n",
    "                        res[t] = 1\n",
    "                X[i:j,:-1] = self.metric(group.doc_id)\n",
    "                X[i:j,-1] = res\n",
    "                i = j\n",
    "            return X, pair_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:21<00:00, 13.53s/it]\n"
     ]
    }
   ],
   "source": [
    "def cross_val_knn():\n",
    "    cos_f = range(4,16,2)\n",
    "    dbscan_f = [0, 1]\n",
    "    fech = [4, 10]\n",
    "    nebrs = [25] #range(5, 30, 5)\n",
    "    f1 = []\n",
    "    for c_f in tqdm(cos_f):\n",
    "        for db_f in dbscan_f:\n",
    "            for f in fech:\n",
    "                c = container(titles_data, c_f)\n",
    "                X, y, groups_train = c.read('data/train_groups.csv')\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(X[:, :c_f])\n",
    "                if db_f==1:\n",
    "                    lft = scaler.transform(X[:,:c_f])\n",
    "                    X = np.hstack((lft, X[:,c_f][:,np.newaxis]))\n",
    "                else:\n",
    "                    X = scaler.transform(X[:,:c_f])\n",
    "                X = np.hstack((X, X_train[:,:f]))\n",
    "                for n in nebrs:\n",
    "                    model = KNeighborsClassifier(n)\n",
    "                    f1.append([cross_val_score(model, X, y, cv=10,scoring = 'f1', n_jobs=-1).mean(),c_f, db_f, f, X.shape, n])\n",
    "    return f1\n",
    "                    \n",
    "f = cross_val_knn()\n",
    "# c = container(titles_data, 8)\n",
    "# c.read('data/train_groups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7478353004768601, 10, 0, 10, (11690, 20), 25]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score, c_f, db_f, f, X.shape, n\n",
    "max(f,key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score, c_f, db_f, f, X.shape, n\n",
    "# max(f,key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [titles_data[i] for i in range(1, len(titles_data))]\n",
    "# titles_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:23<00:00, 41.52s/it]\n"
     ]
    }
   ],
   "source": [
    "model = 0\n",
    "X = 0\n",
    "y = 0\n",
    "def cross_val_randomfor():\n",
    "    c_f = 10\n",
    "    min_samples_s = [ 4, 5]\n",
    "    min_samples_l = [ 4, 5]\n",
    "    n_esti = range(60, 80, 10)\n",
    "    dbscan_f = [0, 1]\n",
    "    db_f = 0\n",
    "    f = 10\n",
    "    fech = [4, 5]\n",
    "    crit = ['gini'] #range(5, 30, 5)\n",
    "    f1 = []\n",
    "    for ss in tqdm(min_samples_s):\n",
    "        for sl in min_samples_l:\n",
    "            for cr in crit:\n",
    "                c = container(titles_data, c_f)\n",
    "                X, y, groups_train = c.read('data/train_groups.csv')\n",
    "                scaler = Normalizer()\n",
    "                scaler.fit(X[:, :c_f])\n",
    "                if db_f==1:\n",
    "                    lft = scaler.transform(X[:,:c_f])\n",
    "                    X = np.hstack((lft, X[:,c_f][:,np.newaxis]))\n",
    "                else:\n",
    "                    X = scaler.transform(X[:,:c_f])\n",
    "                X = np.hstack((X, X_train[:,:f]))\n",
    "                for n in n_esti:\n",
    "                    model = RandomForestClassifier(n_estimators=n, criterion=cr, min_samples_split=ss, min_samples_leaf=sl)\n",
    "                    f1.append([cross_val_score(model, X, y,cv=10,scoring = 'f1').mean(),cr, ss, sl, X.shape, n])\n",
    "    return f1\n",
    "                    \n",
    "f = cross_val_randomfor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = container(titles_data, 10)\n",
    "# X, y, groups_train = c.read('data/train_groups.csv')\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X[:, :10])\n",
    "# model = RandomForestClassifier(n_estimators=80)\n",
    "# X = np.hstack((X, X_train[:,:10]))\n",
    "# cross_val_score(model, X, y, cv=10,scoring = 'f1', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7150423033797212, 'gini', 5, 4, (11690, 20), 70]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f,key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [09:12<00:00, 138.21s/it]\n"
     ]
    }
   ],
   "source": [
    "def cross_val_svm():\n",
    "    c_f = 10\n",
    "    С_reg = [0.5, 1.0, 2.0]\n",
    "    kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "#     min_samples_l = [ 4, 5]\n",
    "    n_esti = range(60, 80, 10)\n",
    "    dbscan_f = [0, 1]\n",
    "    db_f = 0\n",
    "    f = 10\n",
    "    fech = [4, 5]\n",
    "    crit = ['gini'] #range(5, 30, 5)\n",
    "    f1 = []\n",
    "    for k in tqdm(kernel):\n",
    "        for cr in С_reg:\n",
    "            c = container(titles_data, c_f)\n",
    "            X, y, groups_train = c.read('data/train_groups.csv')\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X[:, :c_f])\n",
    "            if db_f==1:\n",
    "                lft = scaler.transform(X[:,:c_f])\n",
    "                X = np.hstack((lft, X[:,c_f][:,np.newaxis]))\n",
    "            else:\n",
    "                X = scaler.transform(X[:,:c_f])\n",
    "            X = np.hstack((X, X_train[:,:f]))\n",
    "            for n in n_esti:\n",
    "                model = SVC(kernel=k, C=cr)\n",
    "                f1.append([cross_val_score(model, X, y,cv=10,scoring = 'f1').mean(),k, X.shape, cr])\n",
    "    return f1\n",
    "                    \n",
    "f = cross_val_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7311443558677078, 'rbf', (11690, 20), 1.0]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f,key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Учимся"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=25)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_f = 10\n",
    "n = 25\n",
    "f = 10 #fech а надо ли?\n",
    "c = container(titles_data, c_f)\n",
    "X_tr, y_tr, groups_tr = c.read('data/train_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr[:, :c_f])\n",
    "X_tr = scaler.transform(X_tr[:,:c_f])\n",
    "# X_tr = np.hstack((X_tr, X_train[:,:f]))\n",
    "model = KNeighborsClassifier(n)\n",
    "model.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 20)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = container(titles_data, c_f)\n",
    "X_te, pair_ids = c.read('data/test_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_te[:, :c_f])\n",
    "X_te = scaler.transform(X_te[:, :c_f])\n",
    "X_te = np.hstack((X_te, X_test[:,:f]))\n",
    "# X_te = np.hstack((X_te, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 20)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predict.csv', 'w') as f:\n",
    "    print('pair_id', 'target', file=f, sep=',')\n",
    "    for pair_id, target in zip(pair_ids, predict):\n",
    "        print(pair_id, int(target), sep=',', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Еще хотелось бы попробовать бустинг и голосование алгоритмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
